{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba3f59c-7a37-4b07-ac7b-4345ff7bcef4",
   "metadata": {},
   "source": [
    "# Project 1\n",
    "**Maddy Bursell, Kim McKeever**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93118b84-ab2e-4010-9368-4c3f2f813510",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import warnings\n",
    "import math\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fe8e3f",
   "metadata": {},
   "source": [
    "## Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2f65",
   "metadata": {},
   "source": [
    "### Read in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd47657-beca-45b4-812b-38e7ea9dcefa",
   "metadata": {},
   "source": [
    "In this project, we will be reading in files that use 2010 US census data. The goal will be to first read in the data and parse it into a format we can use. We will do this using a series of functions for data processing, combining data, and doing cross-validation. Later, we will use the data to make a simple linear regression model and a multiple linear regression model and test whether one works better. \n",
    "\n",
    "First, we read in the initial census data to examine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c53331e-f72f-4098-b820-e36fe5ceef83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area_name</th>\n",
       "      <th>STCOU</th>\n",
       "      <th>EDU010187F</th>\n",
       "      <th>EDU010187D</th>\n",
       "      <th>EDU010187N1</th>\n",
       "      <th>EDU010187N2</th>\n",
       "      <th>EDU010188F</th>\n",
       "      <th>EDU010188D</th>\n",
       "      <th>EDU010188N1</th>\n",
       "      <th>EDU010188N2</th>\n",
       "      <th>...</th>\n",
       "      <th>EDU010194N1</th>\n",
       "      <th>EDU010194N2</th>\n",
       "      <th>EDU010195F</th>\n",
       "      <th>EDU010195D</th>\n",
       "      <th>EDU010195N1</th>\n",
       "      <th>EDU010195N2</th>\n",
       "      <th>EDU010196F</th>\n",
       "      <th>EDU010196D</th>\n",
       "      <th>EDU010196N1</th>\n",
       "      <th>EDU010196N2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40024299</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39967624</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43993459</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44715737</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALABAMA</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>733735</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>728234</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>727989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>736825</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Autauga, AL</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>6829</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7568</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7834</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Baldwin, AL</td>\n",
       "      <td>1003</td>\n",
       "      <td>0</td>\n",
       "      <td>16417</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16465</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19961</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barbour, AL</td>\n",
       "      <td>1005</td>\n",
       "      <td>0</td>\n",
       "      <td>5071</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5098</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5053</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Area_name  STCOU  EDU010187F  EDU010187D  EDU010187N1  EDU010187N2  \\\n",
       "0  UNITED STATES      0           0    40024299            0            0   \n",
       "1        ALABAMA   1000           0      733735            0            0   \n",
       "2    Autauga, AL   1001           0        6829            0            0   \n",
       "3    Baldwin, AL   1003           0       16417            0            0   \n",
       "4    Barbour, AL   1005           0        5071            0            0   \n",
       "\n",
       "   EDU010188F  EDU010188D  EDU010188N1  EDU010188N2  ...  EDU010194N1  \\\n",
       "0           0    39967624            0            0  ...            0   \n",
       "1           0      728234            0            0  ...            0   \n",
       "2           0        6900            0            0  ...            0   \n",
       "3           0       16465            0            0  ...            0   \n",
       "4           0        5098            0            0  ...            0   \n",
       "\n",
       "   EDU010194N2  EDU010195F  EDU010195D  EDU010195N1  EDU010195N2  EDU010196F  \\\n",
       "0            0           0    43993459            0            0           0   \n",
       "1            0           0      727989            0            0           0   \n",
       "2            0           0        7568            0            0           0   \n",
       "3            0           0       19961            0            0           0   \n",
       "4            0           0        5017            0            0           0   \n",
       "\n",
       "   EDU010196D  EDU010196N1  EDU010196N2  \n",
       "0    44715737            0            0  \n",
       "1      736825            0            0  \n",
       "2        7834            0            0  \n",
       "3       20699            0            0  \n",
       "4        5053            0            0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import_data = pd.read_csv(\"https://www4.stat.ncsu.edu/~online/datasets/EDU01a.csv\")\n",
    "import_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339dc0ab",
   "metadata": {},
   "source": [
    "We can see that the data contains many columns and many different kinds of variables within columns. It will be helpful to clean up the data to be more understandable before we begin to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba390b",
   "metadata": {},
   "source": [
    "### Create functions to clean and process the data\n",
    "\n",
    "Functions can be extremely useful when you want to process multiple files in the same way. In this section, we will define 5 different functions that will be called in one wrapper function. All of the functions will work to select only the specific columns we want, create new columns we need, and format the data how we want it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef9d53a",
   "metadata": {},
   "source": [
    "In **func_1** we first want to clean up the data by selecting only certain columns and renaming one column. Our step one will be to rename the \"Area_name\" column as \"area_name\" and to grab all of the columns that end in the letter \"D.\" To do this, it is easiest to create a function that we can use on other datasets down the line. In this function, we take in a dataframe as the input and we have an optional argument to name the new column. The default name of the new column is \"enrollment.\" \n",
    "\n",
    "We only want to maintain a few columns with specific parameters: we want 'Area_name', 'STCOU', and any column that ends with the letter 'D'. Since we want to select all columns ending in \"D\", we are locating those names first. Creating a list of desired columns for our modified data frame by using specific coumn names from the import data and by unpacking the list of column names that end with \"D\" that was generated in the previous line. Next, using the `.loc()` method and the desired columns found in 'col_list', we add those columns to our dataframe. Lastly, we will convert our data into long format where each row has only one enrollment value for the column area_name. This will help to get rid of multiple observations in a given row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45dff0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_1(df_1, col_name = \"enrollment\"):\n",
    "    \n",
    "    # Change the column name\n",
    "    df_2 = df_1.rename(columns = {\"Area_name\":\"area_name\"}, inplace = True)\n",
    "    \n",
    "    # Create a list of the column names and an empty list to grab all of the columns ending in \"D\"\n",
    "    cols=list(df_1)\n",
    "    col_names=[]\n",
    "    \n",
    "    # We will use a for loop to iterate through the columns and grab the column names ending in \"D\" into a list\n",
    "    for x in cols:\n",
    "        if x.endswith(\"D\"):\n",
    "            col_names.append(x)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    # We can subset our original dataframe to grab the columns we want with the list \"col_list\"\n",
    "    col_list=['area_name', 'STCOU', *col_names]\n",
    "    df_2=df_1.loc[:,col_list]\n",
    "    \n",
    "    # Convert the dataframe to long format using pd.melt()\n",
    "    df_2 = df_2.melt(id_vars = [\"area_name\", \"STCOU\"], var_name = \"info\", value_name = col_name)\n",
    "    \n",
    "    return(df_2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b36b86b",
   "metadata": {},
   "source": [
    "In **func_2**, we are taking the output of func_1 and processing it even more. Each variable in the info column holds important information. The last two characters before the \"D\" represent the year. We want to create a new column called \"year\" that stores the year. The first three characters represent the survey and the next four represent the type of value you have from the survey. We want to capture those first seven characters in another column called \"measurement.\"\n",
    "\n",
    "First, we will create new empty columns in our dataframe. Then, we will loop through the dataframe by the row, find the info column value, and parse out the measurment and the year. the value for each row will be added to the new columns of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11fcf6d7-c7d0-4342-8eaf-f64366c8c8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_2(df_2): \n",
    "    \n",
    "    df_2[\"year\"] = np.nan\n",
    "    df_2[\"measurement\"] = np.nan\n",
    "    for row in range(len(df_2)):\n",
    "        thing = df_2.loc[row, \"info\"]\n",
    "        measure = thing[0:7]\n",
    "        if int(thing[7:9])>=23:\n",
    "            yr = \"19\" + str(thing[7:9])\n",
    "        else:\n",
    "            yr= '20'+ str(thing[7:9])\n",
    "        df_2.loc[row,\"year\"] = int(yr)\n",
    "        df_2.loc[row,\"measurement\"] = str(measure)\n",
    "        \n",
    "    return(df_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66a418",
   "metadata": {},
   "source": [
    "In **func_3**, we are only editing the county-level data. This function will be called within a later function (func_5) once we have split our original dataframe into two new dataframes called county_df and state_df. In func_3, we are taking the county_df and creating a new column in it where we have the state for each row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6a716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_3(dataframe):\n",
    "    \n",
    "    dataframe[\"State\"] = dataframe.loc[:,\"area_name\"].apply(lambda x: x[-2:])\n",
    "    \n",
    "    return(dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c351e1",
   "metadata": {},
   "source": [
    "In **func_4**, we are editing only the state-level data. This function will be called in func_5, after we have created the two new dataframes called county_df and state_df. In this function, we will be creating a new column in state_df that included the division for each of the states. We do this by using a function within a function called return_div. This will check if the state is in a certain list, and return the division to the new column in the dataframe using np.vectorize(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19f2d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_4(dataframe):\n",
    "    \n",
    "    def return_div(val):\n",
    "        div1 = [\"CONNECTICUT\", 'MAINE','MASSACHUSETTS','NEW HAMPSHIRE','RHODE ISLAND','VERMONT']\n",
    "        div2 = [\"NEW JERSEY\",'NEW YORK','PENNSYLVANIA']\n",
    "        div3 = ['ILLINOIS','INDIANA','MICHIGAN','OHIO','WISCONSIN']\n",
    "        div4 = ['IOWA','KANSAS','MINNESOTA','MISSOURI','NEBRASKA','NORTH DAKOTA','SOUTH DAKOTA']\n",
    "        div5 = ['DELAWARE','FLORIDA','GEORGIA','MARYLAND','NORTH CAROLINA','SOUTH CAROLINA','VIRGINIA','DISTRICT OF COLUMBIA','WEST VIRGINIA']\n",
    "        div6 = ['ALABAMA','KENTUCKY','MISSISSIPPI','TENNESSEE']\n",
    "        div7 = ['ARKANSAS','LOUISIANA','OKLAHOMA','TEXAS']\n",
    "        div8 = ['ARIZONA','COLORADO','IDAHO','NEVADA','MONTANA','NEW MEXICO','UTAH','WYOMING']\n",
    "        div9 = ['ALASKA','CALIFORNIA','HAWAII','OREGON','WASHINGTON']\n",
    "        val = val.upper()\n",
    "    \n",
    "        if val in div1:\n",
    "            return(\"Division 1\")\n",
    "        elif val in div2:\n",
    "            return(\"Division 2\")\n",
    "        elif val in div3:\n",
    "            return(\"Division 3\")\n",
    "        elif val in div4:\n",
    "            return(\"Division 4\")\n",
    "        elif val in div5:\n",
    "            return(\"Division 5\")\n",
    "        elif val in div6:\n",
    "            return(\"Division 6\")\n",
    "        elif val in div7:\n",
    "            return(\"Division 7\")\n",
    "        elif val in div8:\n",
    "            return(\"Division 8\")\n",
    "        elif val in div9:\n",
    "            return(\"Division 9\")\n",
    "        else:\n",
    "            return(\"ERROR\")\n",
    "        \n",
    "    dataframe[\"division\"] = np.vectorize(return_div)(dataframe[\"area_name\"])\n",
    "    return(dataframe)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109afc9",
   "metadata": {},
   "source": [
    "In **func_5**, we are processing the output from func_2 and creating two new dataframes. We can see that the \"area_name\" column holds two different kinds of values: states and counties of states. We would like to create two separate dataframes that hold the state-level data and the county-level data. To do that, we will create an indexing vector using a lambda function. The lambda function is searching the string within each row, in the 'area_name' column, searching to match if the fourth character of from the end is a `','`. This information is passed into a new column on the data frame that expresses a boolean to identify whether or not the row represented is part of a county or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062d098d-8e99-4b24-a347-0af0e7978a11",
   "metadata": {},
   "source": [
    "Now that there is a new column on `census_df` to index whether or not an area represents a county or not, the two different dataframes are created. The county data frame is generated using `.loc()` and the state dataframe is generated by running `np.logical_not()` method from numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af3f485-ca33-41a0-b576-ca2045611a08",
   "metadata": {},
   "source": [
    "The State information was then pulled from the `area_name` column to be held in its own unique column called \"State\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befba0f6",
   "metadata": {},
   "source": [
    "Once we have the two new dataframes, we will call **func_3** on county_df which will add a column called \"state\" specifying the state in each row. We will also call **func_4** on state_df which will add a new column in that dataframe that specifies the division of each state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef225780",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_5(dataframe):\n",
    "    \n",
    "    dataframe[\"is_county\"] = dataframe[\"area_name\"].apply(lambda x: x[-4] == \",\")\n",
    "    county_df = dataframe.loc[dataframe[\"is_county\"]]\n",
    "    state_df = dataframe.loc[np.logical_not(dataframe[\"is_county\"])]\n",
    "    \n",
    "    county_df = func_3(county_df)\n",
    "    final_county_df = county_df[[\"area_name\", \"STCOU\", \"enrollment\", \"year\", \"measurement\", \"State\"]]\n",
    "    state_df = func_4(state_df)\n",
    "    final_state_df = state_df[[\"area_name\", \"STCOU\", \"enrollment\", \"year\", \"measurement\", \"division\"]]\n",
    "    \n",
    "    return(final_county_df, final_state_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2e034b",
   "metadata": {},
   "source": [
    "### Create a wrapper function that will call all of our previously defined functions\n",
    "\n",
    "If we have several functions that function continuously, meaning the output of one function becomes the input of the next function, we can easily run all of these functions at once by putting them within another function. This saves time and energy, and keeps you from making mistakes while typing in the input for each individual function. In our wrapper function, we give the argument for the data to read in and then all of the functions will run, leaving us with our desired output. Because the wrapper function does most of the data processing steps, we have named it \"data_processing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dfe1b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(URL, col_name = \"enrollment\"):\n",
    "    \n",
    "    import_data = pd.read_csv(URL)\n",
    "    census_df = func_1(import_data)\n",
    "    census_df_2 = func_2(census_df)\n",
    "    county_df, non_county_df = func_5(census_df_2)\n",
    "    \n",
    "    return([county_df, non_county_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e0ac8",
   "metadata": {},
   "source": [
    "We named the output from the first URL \"file_1.\" We called the wrapper function and used the URL as the argument. The output of our function is a list of the two dataframes. The first dataframe includes county-level data. The second dataframe includes non-county data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31428a4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            area_name  STCOU  enrollment    year measurement State\n",
       " 2         Autauga, AL   1001        6829  1987.0     EDU0101    AL\n",
       " 3         Baldwin, AL   1003       16417  1987.0     EDU0101    AL\n",
       " 4         Barbour, AL   1005        5071  1987.0     EDU0101    AL\n",
       " 5            Bibb, AL   1007        3557  1987.0     EDU0101    AL\n",
       " 6          Blount, AL   1009        7319  1987.0     EDU0101    AL\n",
       " ...               ...    ...         ...     ...         ...   ...\n",
       " 31975  Sweetwater, WY  56037        9599  1996.0     EDU0101    WY\n",
       " 31976       Teton, WY  56039        2226  1996.0     EDU0101    WY\n",
       " 31977       Uinta, WY  56041        5750  1996.0     EDU0101    WY\n",
       " 31978    Washakie, WY  56043        1900  1996.0     EDU0101    WY\n",
       " 31979      Weston, WY  56045        1479  1996.0     EDU0101    WY\n",
       " \n",
       " [31450 rows x 6 columns],\n",
       "            area_name  STCOU  enrollment    year measurement    division\n",
       " 0      UNITED STATES      0    40024299  1987.0     EDU0101       ERROR\n",
       " 1            ALABAMA   1000      733735  1987.0     EDU0101  Division 6\n",
       " 69            ALASKA   2000      102872  1987.0     EDU0101  Division 9\n",
       " 99           ARIZONA   4000      609411  1987.0     EDU0101  Division 8\n",
       " 115         ARKANSAS   5000      429260  1987.0     EDU0101  Division 7\n",
       " ...              ...    ...         ...     ...         ...         ...\n",
       " 31650       VIRGINIA  51000     1079854  1996.0     EDU0101  Division 5\n",
       " 31787     WASHINGTON  53000      956572  1996.0     EDU0101  Division 9\n",
       " 31827  WEST VIRGINIA  54000      307112  1996.0     EDU0101  Division 5\n",
       " 31883      WISCONSIN  55000      870175  1996.0     EDU0101  Division 3\n",
       " 31956        WYOMING  56000       99859  1996.0     EDU0101  Division 8\n",
       " \n",
       " [530 rows x 6 columns]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_1 = data_processing(\"https://www4.stat.ncsu.edu/~online/datasets/EDU01a.csv\")\n",
    "file_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebaa338",
   "metadata": {},
   "source": [
    "We named the output from our second URL \"file_2.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "393a3a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[            area_name  STCOU  enrollment    year measurement State\n",
       " 2         Autauga, AL   1001        8099  1997.0     EDU0101    AL\n",
       " 3         Baldwin, AL   1003       21410  1997.0     EDU0101    AL\n",
       " 4         Barbour, AL   1005        5100  1997.0     EDU0101    AL\n",
       " 5            Bibb, AL   1007        3717  1997.0     EDU0101    AL\n",
       " 6          Blount, AL   1009        7816  1997.0     EDU0101    AL\n",
       " ...               ...    ...         ...     ...         ...   ...\n",
       " 31975  Sweetwater, WY  56037        6964  2006.0     EDU0152    WY\n",
       " 31976       Teton, WY  56039        2264  2006.0     EDU0152    WY\n",
       " 31977       Uinta, WY  56041        4298  2006.0     EDU0152    WY\n",
       " 31978    Washakie, WY  56043        1410  2006.0     EDU0152    WY\n",
       " 31979      Weston, WY  56045        1076  2006.0     EDU0152    WY\n",
       " \n",
       " [31450 rows x 6 columns],\n",
       "            area_name  STCOU  enrollment    year measurement    division\n",
       " 0      UNITED STATES      0    44534459  1997.0     EDU0101       ERROR\n",
       " 1            ALABAMA   1000      737386  1997.0     EDU0101  Division 6\n",
       " 69            ALASKA   2000      129919  1997.0     EDU0101  Division 9\n",
       " 99           ARIZONA   4000      798069  1997.0     EDU0101  Division 8\n",
       " 115         ARKANSAS   5000      459787  1997.0     EDU0101  Division 7\n",
       " ...              ...    ...         ...     ...         ...         ...\n",
       " 31650       VIRGINIA  51000     1220440  2006.0     EDU0152  Division 5\n",
       " 31787     WASHINGTON  53000     1026774  2006.0     EDU0152  Division 9\n",
       " 31827  WEST VIRGINIA  54000      281938  2006.0     EDU0152  Division 5\n",
       " 31883      WISCONSIN  55000      876700  2006.0     EDU0152  Division 3\n",
       " 31956        WYOMING  56000       85193  2006.0     EDU0152  Division 8\n",
       " \n",
       " [530 rows x 6 columns]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_2 = data_processing(\"https://www4.stat.ncsu.edu/~online/datasets/EDU01b.csv\")\n",
    "file_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0822a5da",
   "metadata": {},
   "source": [
    "## Part 2: Combine Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fd69c5",
   "metadata": {},
   "source": [
    "### Combine county and state data from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c15c4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(*files):\n",
    "    county_data = list(map(lambda x: x[0], files))\n",
    "    non_state_data = map(lambda x: x[1], files)\n",
    "\n",
    "    combine_county = functools.reduce(lambda x,y: pd.concat([x,y]), county_data)\n",
    "    combine_non_county = functools.reduce(lambda x,y: pd.concat([x,y]), non_county_data)\n",
    "    \n",
    "    return([combine_county, combine_non_county])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f1f199",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'non_county_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cq/wkd76m4s1mb9xtzp3mp6kf140000gn/T/ipykernel_15483/3899543276.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcombined_enrollment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcombined_enrollment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/cq/wkd76m4s1mb9xtzp3mp6kf140000gn/T/ipykernel_15483/1290435148.py\u001b[0m in \u001b[0;36mcombine_data\u001b[0;34m(*files)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcombine_county\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounty_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcombine_non_county\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_county_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombine_county\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine_non_county\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'non_county_data' is not defined"
     ]
    }
   ],
   "source": [
    "combined_enrollment = combine_data(file_1, file_2)\n",
    "combined_enrollment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d46ed9",
   "metadata": {},
   "source": [
    "### Check if it generalizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01825ab",
   "metadata": {},
   "source": [
    "We have seen that our data processing functions work on the files labeled \"EDU01a\" and \"EDU01b\", now we will check to see if our functions are generalized enough to work on multiple new files. We will import 4 new files using their URLs. \n",
    "\n",
    "Let's begin by running our wrapping function \"data_processing\" on the four new files to create 4 lists of county and state dataframes. We will name these additional files \"test\" so that we can distinguish them from the main two files we have been working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67f5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = data_processing(\"https://www4.stat.ncsu.edu/~online/datasets/PST01a.csv\")\n",
    "test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_2 = data_processing(\"https://www4.stat.ncsu.edu/~online/datasets/PST01b.csv\")\n",
    "test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38f47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_3 = data_processing(\"https://www4.stat.ncsu.edu/~online/datasets/PST01c.csv\")\n",
    "test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7288943",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_4 = data_processing(\"https://www4.stat.ncsu.edu/~online/datasets/PST01d.csv\")\n",
    "test_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99586a52",
   "metadata": {},
   "source": [
    "Now that we have the 4 lists of county and state dataframes from each of the 4 test datasets, we can combine all of the county data into one dataframe and combine all of the state data into one dataframe using our combine_data function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3579b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_county, test_non_county = combine_data(test_1, test_2, test_3, test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d986dba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_county.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c5844",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_non_county.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b9476",
   "metadata": {},
   "source": [
    "## Part 3: Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5a0359",
   "metadata": {},
   "source": [
    "### Subset the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bde1c08",
   "metadata": {},
   "source": [
    "In the next section, we will analyze our data. We want to look only at the non-county data, so we will subset our list of dataframes that we got as output from the combine_data function we made. That function returns a list with the combined county dataframe as the first element and the combined non-county dataframe as the second element. Because list indices start at 0, we will subset the second element using [1]. We will name this new variable **combined_non_county** so we know that it holds the combined dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308faae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_non_county = combined_enrollment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_non_county"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8945ca3d",
   "metadata": {},
   "source": [
    "Now that we just have the non-county data, we also want to remove any rows where there is \"ERROR\" in the division column. This means that we will only be looking at states. We will subset our dataframe by locating the rows where the division column values do not equal \"ERROR.\" We will rename this dataframe to **combined_state** because we are now dealing with just states and not all non-county data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092aa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_state = combined_non_county.loc[combined_non_county[\"division\"] != \"ERROR\",[\"enrollment\", \"year\", \"division\"]]\n",
    "combined_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b175715",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X = combined_state[\"year\"].values.reshape(-1,1), y = combined_state[\"enrollment\"].values)\n",
    "print(reg.intercept_, reg.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd64ffb-3d57-406d-b593-e6357db0d972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subsetting three years of data\n",
    "subset_3yr=combined_state.loc[combined_state['year'] <(min(combined_state['year']+3))]\n",
    "reg=linear_model.LinearRegression()\n",
    "reg.fit(X=subset_3yr[\"year\"].values.reshape(-1,1), y=subset_3yr[\"enrollment\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17d0e4f-4c89-474c-bf06-c546ca126e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimating intercept and slope of simple linear regression\n",
    "print(round(reg.intercept_,3), round(reg.coef_[0],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c2ec7-6030-4814-bdaf-0382d864fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting future data\n",
    "pred_3yr=reg.predict([[1990]])\n",
    "print(pred_3yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac9a6b-edac-479a-a27b-a8d5b64cd550",
   "metadata": {},
   "source": [
    "Chart of simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855638d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x = combined_state[\"year\"], y = combined_state[\"enrollment\"], scatter_kws = {\"s\":2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dd66e",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4cb91-2147-401e-87c9-bc95cd4044c0",
   "metadata": {},
   "source": [
    "In this section, we will be creating a SLR model using **year** to predict **enrollment**, and a MLR model using **year** and **division** to predict **enrollment.** \n",
    "\n",
    "To do this, we are creating a function to take an input of defined predictor and response variables, and the last year to be considered. This function will use certain years to fit the data and then predict the next year enrollment. We will start by training the model on the first three years to predict the fourth year. Then, we will train on four years and predict the fifth year, cycling through until we have predicted the last year included in the dataset. The output of the function will be the mean square error and mean error of the prediction for the next year. This function is called **mse_LR**. \n",
    "\n",
    "We want this because we are going to test if a simple linear regression model or a multiple linear regression model fits the data better. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c769550",
   "metadata": {},
   "source": [
    "To do MLR, we will need to create the dummy variables. We will do this using the **division** column. We will only include the first 8 variables, leaving off \"division 9\" because it is redundant given all others. We use the pd.get_dummies() method to create the dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a571cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data = combined_state[\"division\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1982a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_state[[\"dummy_div_1\", \"dummy_div_2\", \"dummy_div_3\", \"dummy_div_4\", \"dummy_div_5\", \"dummy_div_6\", \"dummy_div_7\", \"dummy_div_8\"]] = pd.get_dummies(data = combined_state[\"division\"])[[\"Division 1\", \"Division 2\", \"Division 3\", \"Division 4\", \"Division 5\", \"Division 6\", \"Division 7\", \"Division 8\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef01e7e",
   "metadata": {},
   "source": [
    "Next, we have to define the predictors and response variables for both SLR and MLR that we will use an input for our function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076904eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SLR_predictors = combined_state[[\"year\"]]\n",
    "MLR_predictors = combined_state[[\"year\", \"dummy_div_1\", \"dummy_div_2\", \"dummy_div_3\", \"dummy_div_4\", \"dummy_div_5\", \"dummy_div_6\", \"dummy_div_7\", \"dummy_div_8\"]]\n",
    "both_response = combined_state[\"enrollment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9dfd3f-0319-45b8-b55b-04eee29d8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_LR(predictors, response, last_year):\n",
    "    \n",
    "    x_train=predictors.loc[predictors['year'] <=(last_year)]\n",
    "    x_test=predictors.loc[predictors['year']==last_year+1]\n",
    "    y_test = response.loc[predictors['year'] <=(last_year)]\n",
    "    y_train = response.loc[predictors['year']==last_year+1]\n",
    "    reg=linear_model.LinearRegression()\n",
    "    reg.fit(X=x_train.values.reshape(-1,1), y=y_train.values)\n",
    "    preds = reg.predict(x_test.values.reshape(-1,1))\n",
    "    MSE = mean_squared_error(y_test,preds)\n",
    "    \n",
    "    return(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5919b296",
   "metadata": {},
   "source": [
    "Our first function outputs the sequential MSE for each step we are training and predicting. However, we want to know the total MSE of predicting all the years after the 3rd year. To calculate the total MSE, we created a function called **sum_MSE** that will call our **mse_LR** within it and sum the MSE calculated for each step. This function will take in the predictors, response, and the first_year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82710681",
   "metadata": {},
   "source": [
    "To see all of the years that are in the **year** column of the combined_state dataframe, we can use the .unique() method to print out all of the unique values in that column. Doing so we can see that the first year in the dataset or the earliest year is 1987. We will use this as input for our next function where we need the first year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3465ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_state.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d7798b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_MSE(predictors, response, first_year):\n",
    "    \n",
    "    years_list = list(combined_state.year.unique())\n",
    "    MSE = 0\n",
    "    for first_year in years_list:\n",
    "        if first_year < 1989.0:\n",
    "            print(\"Error: you have not provided more than three years to calculate MSE\")\n",
    "            continue\n",
    "        else:\n",
    "            MSE += mse_LR(predictors, response, first_year)\n",
    "            \n",
    "    return(\"MSE:\", MSE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8da5cb3-0dd9-4b01-b1de-bdb45313fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_LR(SLR_predictors, both_response, last_year=1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee5c15c",
   "metadata": {},
   "source": [
    "Now that we have defined both of our functions, we can call them to calculate the MSE for our simple linear regression model (SLR_MSE) and our multuple linear regression model (MLR_MSE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae059e62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SLR_predictors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cq/wkd76m4s1mb9xtzp3mp6kf140000gn/T/ipykernel_15483/2899642670.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSLR_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSLR_predictors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboth_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1987\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mSLR_MSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SLR_predictors' is not defined"
     ]
    }
   ],
   "source": [
    "SLR_MSE = sum_MSE(SLR_predictors, both_response, first_year = 1987)\n",
    "SLR_MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d114f-3031-406b-a8d5-d8b7fd5b9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLR_MSE = sum_MSE(MLR_predictors, both_response, first_year = 1987)\n",
    "MLR_MSE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
